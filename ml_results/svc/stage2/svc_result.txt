➜  cmup git:(master) ✗ ./cmup.py trainEmotionClassifier --load
[CONFIG] reading config: './conf/cmup_conf.cfg'
[CLASSIFIER] loading train data
[LOADER] label: Angry_all
[LOADER] n features: 101
[LOADER] m samples: 507
[LOADER] label: Happy_all
[LOADER] n features: 101
[LOADER] m samples: 584
[LOADER] label: Relax_all
[LOADER] n features: 101
[LOADER] m samples: 584
[LOADER] label: Sad_all
[LOADER] n features: 101
[LOADER] m samples: 597
[LOADER] making feature matrix and feature labels vector
[LOADER] feature matrix shape: 2272 x 101
[LOADER] feature labels shape: 2272
[CLASSIFIER] train data shape
(2272, 101)
[CLASSIFIER] loading test data
[LOADER] label: Angry_all
[LOADER] n features: 101
[LOADER] m samples: 126
[LOADER] label: Happy_all
[LOADER] n features: 101
[LOADER] m samples: 146
[LOADER] label: Relax_all
[LOADER] n features: 101
[LOADER] m samples: 146
[LOADER] label: Sad_all
[LOADER] n features: 101
[LOADER] m samples: 149
[LOADER] making feature matrix and feature labels vector
[LOADER] feature matrix shape: 567 x 101
[LOADER] feature labels shape: 567
[CLASSIFIER] test data shape
(567, 101)
[LOADER] perform feature standartization (using sklearn.preprocessing.StandardScaler class)
[[  3.60085068e+01   9.79786754e-01   9.47808504e-01 ...,   6.73913002e-01
    2.79889989e+00   1.01000000e+02]
 [  1.19133797e+01   9.68645930e-01   5.77967942e-01 ...,   4.74999964e-01
    3.96510816e+00   8.90000000e+01]
 [  1.87092533e+01   9.39435542e-01   5.39758086e-01 ...,   4.30894315e-01
    4.26911879e+00   1.26000000e+02]
 ..., 
 [  1.15001936e+01   8.74582946e-01   8.94781649e-01 ...,   2.28260815e-01
    4.20241404e+00   9.80000000e+01]
 [  2.12920208e+01   9.64270353e-01   8.69289339e-01 ...,   1.55172467e-01
    2.93672514e+00   1.57000000e+02]
 [  1.54488220e+01   6.38340116e-01   9.30681050e-01 ...,   4.48529422e-01
    3.39472437e+00   1.44000000e+02]]
[[ 2.14064642  0.56590879 -0.015346   ...,  1.99085315 -0.70583633
  -0.88586098]
 [-0.80162041  0.49203325 -0.89852377 ...,  0.82223818  0.42101711
  -1.36385563]
 [ 0.02822678  0.29833724 -0.98976875 ...,  0.56311729  0.7147686
   0.10996121]
 ..., 
 [-0.85207475 -0.1317046  -0.14197391 ..., -0.62735542  0.65031487
  -1.00535964]
 [ 0.34360968  0.46301851 -0.20284945 ..., -1.05674978 -0.57266213
   1.34478073]
 [-0.36990595 -1.6982464  -0.05624629 ...,  0.66672362 -0.13011852
   0.82695319]]
[LOADER] perform feature standartization (using sklearn.preprocessing.StandardScaler class)
[[  11.60788822    0.97211963    1.05534041 ...,    0.            5.06467581
   157.        ]
 [  12.40735054    0.68405718    0.62107021 ...,    0.26530612
     2.08291769   99.        ]
 [   8.3523035     0.67839611    1.61179793 ...,    0.54736841
     2.05117822   99.        ]
 ..., 
 [  11.99690628    0.94270056    1.22871256 ...,    0.            4.19834995
   161.        ]
 [  14.6537571     0.96147597    1.34589136 ...,    0.35593218
     1.36824691  129.        ]
 [  18.75434494    0.97158396    1.5354923  ...,    0.39830512
     2.94542265  129.        ]]
[[-0.83892411  0.51506758  0.24143976 ..., -1.96838871  1.48347878
   1.34478073]
 [-0.74130141 -1.39509355 -0.79559574 ..., -0.40971411 -1.39765701
  -0.96552676]
 [-1.23646503 -1.43263248  1.57025807 ...,  1.2474031  -1.42832541
  -0.96552676]
 ..., 
 [-0.79142094  0.31998782  0.65545175 ..., -1.96838871  0.64638793
   1.50411228]
 [-0.46699169  0.44448876  0.93527425 ...,  0.12271439 -2.08821053
   0.22945987]
 [ 0.03373294  0.51151554  1.38804054 ...,  0.37165562 -0.56425812
   0.22945987]]
[LOADER] perform PCA feature selction [n_components: 2]
[LABEL] PCA Explained Variance: [ 0.16188542  0.0776623 ]
[[ -5.78227107  -1.82615052]
 [  0.61739954   3.66721075]
 [ -1.47701743   1.83974804]
 [ 11.12457724   3.17389821]
 [  2.66023738  -4.54810508]]
[LOADER] perform PCA feature selction [n_components: 2]
[LABEL] PCA Explained Variance: [ 0.16188542  0.0776623 ]
[[  2.96388865   4.04726415]
 [  2.75840092   4.50807219]
 [ 11.48430691  -3.84380168]
 [  5.07433244   4.40173398]
 [ -2.20595623  -3.06971428]]
[LOADER] perform label encoding
[LOADER] perform label encoding
# Tuning hyper-parameters for precision
()
/usr/local/lib/python2.7/site-packages/sklearn/metrics/classification.py:1113: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)
Best parameters set found on development set:
()
{'kernel': 'rbf', 'C': 0.025, 'gamma': 0.007}
()
Grid scores on development set:
()
0.470 (+/-0.049) for {'kernel': 'rbf', 'C': 0.025, 'gamma': 0.1}
0.405 (+/-0.225) for {'kernel': 'rbf', 'C': 0.025, 'gamma': 0.5}
0.492 (+/-0.044) for {'kernel': 'rbf', 'C': 0.025, 'gamma': 0.01}
0.478 (+/-0.039) for {'kernel': 'rbf', 'C': 0.025, 'gamma': 0.03}
0.478 (+/-0.040) for {'kernel': 'rbf', 'C': 0.025, 'gamma': 0.05}
0.488 (+/-0.054) for {'kernel': 'rbf', 'C': 0.025, 'gamma': 0.07}
0.132 (+/-0.041) for {'kernel': 'rbf', 'C': 0.025, 'gamma': 0.001}
0.187 (+/-0.003) for {'kernel': 'rbf', 'C': 0.025, 'gamma': 0.003}
0.326 (+/-0.232) for {'kernel': 'rbf', 'C': 0.025, 'gamma': 0.005}
0.552 (+/-0.134) for {'kernel': 'rbf', 'C': 0.025, 'gamma': 0.007}
0.066 (+/-0.000) for {'kernel': 'rbf', 'C': 0.025, 'gamma': 0.0001}
0.479 (+/-0.056) for {'kernel': 'rbf', 'C': 1, 'gamma': 0.1}
0.472 (+/-0.066) for {'kernel': 'rbf', 'C': 1, 'gamma': 0.5}
0.485 (+/-0.050) for {'kernel': 'rbf', 'C': 1, 'gamma': 0.01}
0.489 (+/-0.045) for {'kernel': 'rbf', 'C': 1, 'gamma': 0.03}
0.484 (+/-0.054) for {'kernel': 'rbf', 'C': 1, 'gamma': 0.05}
0.481 (+/-0.052) for {'kernel': 'rbf', 'C': 1, 'gamma': 0.07}
0.468 (+/-0.053) for {'kernel': 'rbf', 'C': 1, 'gamma': 0.001}
0.480 (+/-0.053) for {'kernel': 'rbf', 'C': 1, 'gamma': 0.003}
0.484 (+/-0.056) for {'kernel': 'rbf', 'C': 1, 'gamma': 0.005}
0.484 (+/-0.054) for {'kernel': 'rbf', 'C': 1, 'gamma': 0.007}
0.475 (+/-0.221) for {'kernel': 'rbf', 'C': 1, 'gamma': 0.0001}
0.470 (+/-0.061) for {'kernel': 'rbf', 'C': 10, 'gamma': 0.1}
0.454 (+/-0.070) for {'kernel': 'rbf', 'C': 10, 'gamma': 0.5}
0.491 (+/-0.054) for {'kernel': 'rbf', 'C': 10, 'gamma': 0.01}
0.483 (+/-0.053) for {'kernel': 'rbf', 'C': 10, 'gamma': 0.03}
0.478 (+/-0.051) for {'kernel': 'rbf', 'C': 10, 'gamma': 0.05}
0.477 (+/-0.053) for {'kernel': 'rbf', 'C': 10, 'gamma': 0.07}
0.486 (+/-0.055) for {'kernel': 'rbf', 'C': 10, 'gamma': 0.001}
0.491 (+/-0.056) for {'kernel': 'rbf', 'C': 10, 'gamma': 0.003}
0.486 (+/-0.052) for {'kernel': 'rbf', 'C': 10, 'gamma': 0.005}
0.487 (+/-0.054) for {'kernel': 'rbf', 'C': 10, 'gamma': 0.007}
0.460 (+/-0.056) for {'kernel': 'rbf', 'C': 10, 'gamma': 0.0001}
0.473 (+/-0.039) for {'kernel': 'rbf', 'C': 100, 'gamma': 0.1}
0.415 (+/-0.093) for {'kernel': 'rbf', 'C': 100, 'gamma': 0.5}
0.489 (+/-0.048) for {'kernel': 'rbf', 'C': 100, 'gamma': 0.01}
0.481 (+/-0.047) for {'kernel': 'rbf', 'C': 100, 'gamma': 0.03}
0.476 (+/-0.046) for {'kernel': 'rbf', 'C': 100, 'gamma': 0.05}
0.471 (+/-0.063) for {'kernel': 'rbf', 'C': 100, 'gamma': 0.07}
0.492 (+/-0.044) for {'kernel': 'rbf', 'C': 100, 'gamma': 0.001}
0.493 (+/-0.060) for {'kernel': 'rbf', 'C': 100, 'gamma': 0.003}
0.492 (+/-0.054) for {'kernel': 'rbf', 'C': 100, 'gamma': 0.005}
0.495 (+/-0.055) for {'kernel': 'rbf', 'C': 100, 'gamma': 0.007}
0.460 (+/-0.063) for {'kernel': 'rbf', 'C': 100, 'gamma': 0.0001}
0.455 (+/-0.047) for {'kernel': 'rbf', 'C': 1000, 'gamma': 0.1}
0.399 (+/-0.098) for {'kernel': 'rbf', 'C': 1000, 'gamma': 0.5}
0.485 (+/-0.052) for {'kernel': 'rbf', 'C': 1000, 'gamma': 0.01}
0.475 (+/-0.044) for {'kernel': 'rbf', 'C': 1000, 'gamma': 0.03}
0.469 (+/-0.056) for {'kernel': 'rbf', 'C': 1000, 'gamma': 0.05}
0.471 (+/-0.041) for {'kernel': 'rbf', 'C': 1000, 'gamma': 0.07}
0.489 (+/-0.061) for {'kernel': 'rbf', 'C': 1000, 'gamma': 0.001}
0.494 (+/-0.062) for {'kernel': 'rbf', 'C': 1000, 'gamma': 0.003}
0.489 (+/-0.053) for {'kernel': 'rbf', 'C': 1000, 'gamma': 0.005}
0.487 (+/-0.047) for {'kernel': 'rbf', 'C': 1000, 'gamma': 0.007}
0.487 (+/-0.059) for {'kernel': 'rbf', 'C': 1000, 'gamma': 0.0001}
0.457 (+/-0.060) for {'kernel': 'linear', 'C': 0.025}
0.457 (+/-0.062) for {'kernel': 'linear', 'C': 1}
0.457 (+/-0.061) for {'kernel': 'linear', 'C': 10}
0.456 (+/-0.061) for {'kernel': 'linear', 'C': 100}
0.455 (+/-0.063) for {'kernel': 'linear', 'C': 1000}
()
Detailed classification report:
()
The model is trained on the full development set.
The scores are computed on the full evaluation set.
()
             precision    recall  f1-score   support

          0       0.74      0.34      0.47       126
          1       0.40      0.75      0.52       146
          2       0.56      0.07      0.12       146
          3       0.40      0.57      0.47       149

avg / total       0.51      0.44      0.39       567

()
# Tuning hyper-parameters for recall
()
Best parameters set found on development set:
()
{'kernel': 'rbf', 'C': 10, 'gamma': 0.001}
()
Grid scores on development set:
()
0.430 (+/-0.044) for {'kernel': 'rbf', 'C': 0.025, 'gamma': 0.1}
0.345 (+/-0.020) for {'kernel': 'rbf', 'C': 0.025, 'gamma': 0.5}
0.419 (+/-0.035) for {'kernel': 'rbf', 'C': 0.025, 'gamma': 0.01}
0.446 (+/-0.047) for {'kernel': 'rbf', 'C': 0.025, 'gamma': 0.03}
0.446 (+/-0.045) for {'kernel': 'rbf', 'C': 0.025, 'gamma': 0.05}
0.447 (+/-0.050) for {'kernel': 'rbf', 'C': 0.025, 'gamma': 0.07}
0.272 (+/-0.025) for {'kernel': 'rbf', 'C': 0.025, 'gamma': 0.001}
0.350 (+/-0.008) for {'kernel': 'rbf', 'C': 0.025, 'gamma': 0.003}
0.357 (+/-0.016) for {'kernel': 'rbf', 'C': 0.025, 'gamma': 0.005}
0.386 (+/-0.022) for {'kernel': 'rbf', 'C': 0.025, 'gamma': 0.007}
0.250 (+/-0.000) for {'kernel': 'rbf', 'C': 0.025, 'gamma': 0.0001}
0.451 (+/-0.055) for {'kernel': 'rbf', 'C': 1, 'gamma': 0.1}
0.453 (+/-0.068) for {'kernel': 'rbf', 'C': 1, 'gamma': 0.5}
0.452 (+/-0.048) for {'kernel': 'rbf', 'C': 1, 'gamma': 0.01}
0.456 (+/-0.046) for {'kernel': 'rbf', 'C': 1, 'gamma': 0.03}
0.453 (+/-0.053) for {'kernel': 'rbf', 'C': 1, 'gamma': 0.05}
0.452 (+/-0.053) for {'kernel': 'rbf', 'C': 1, 'gamma': 0.07}
0.459 (+/-0.053) for {'kernel': 'rbf', 'C': 1, 'gamma': 0.001}
0.454 (+/-0.047) for {'kernel': 'rbf', 'C': 1, 'gamma': 0.003}
0.455 (+/-0.049) for {'kernel': 'rbf', 'C': 1, 'gamma': 0.005}
0.454 (+/-0.051) for {'kernel': 'rbf', 'C': 1, 'gamma': 0.007}
0.355 (+/-0.009) for {'kernel': 'rbf', 'C': 1, 'gamma': 0.0001}
0.444 (+/-0.064) for {'kernel': 'rbf', 'C': 10, 'gamma': 0.1}
0.439 (+/-0.079) for {'kernel': 'rbf', 'C': 10, 'gamma': 0.5}
0.455 (+/-0.051) for {'kernel': 'rbf', 'C': 10, 'gamma': 0.01}
0.452 (+/-0.053) for {'kernel': 'rbf', 'C': 10, 'gamma': 0.03}
0.450 (+/-0.051) for {'kernel': 'rbf', 'C': 10, 'gamma': 0.05}
0.449 (+/-0.055) for {'kernel': 'rbf', 'C': 10, 'gamma': 0.07}
0.459 (+/-0.047) for {'kernel': 'rbf', 'C': 10, 'gamma': 0.001}
0.457 (+/-0.049) for {'kernel': 'rbf', 'C': 10, 'gamma': 0.003}
0.451 (+/-0.050) for {'kernel': 'rbf', 'C': 10, 'gamma': 0.005}
0.452 (+/-0.050) for {'kernel': 'rbf', 'C': 10, 'gamma': 0.007}
0.458 (+/-0.061) for {'kernel': 'rbf', 'C': 10, 'gamma': 0.0001}
0.448 (+/-0.044) for {'kernel': 'rbf', 'C': 100, 'gamma': 0.1}
0.409 (+/-0.091) for {'kernel': 'rbf', 'C': 100, 'gamma': 0.5}
0.453 (+/-0.048) for {'kernel': 'rbf', 'C': 100, 'gamma': 0.01}
0.452 (+/-0.048) for {'kernel': 'rbf', 'C': 100, 'gamma': 0.03}
0.447 (+/-0.047) for {'kernel': 'rbf', 'C': 100, 'gamma': 0.05}
0.442 (+/-0.065) for {'kernel': 'rbf', 'C': 100, 'gamma': 0.07}
0.455 (+/-0.037) for {'kernel': 'rbf', 'C': 100, 'gamma': 0.001}
0.454 (+/-0.056) for {'kernel': 'rbf', 'C': 100, 'gamma': 0.003}
0.454 (+/-0.050) for {'kernel': 'rbf', 'C': 100, 'gamma': 0.005}
0.454 (+/-0.052) for {'kernel': 'rbf', 'C': 100, 'gamma': 0.007}
0.453 (+/-0.064) for {'kernel': 'rbf', 'C': 100, 'gamma': 0.0001}
0.435 (+/-0.049) for {'kernel': 'rbf', 'C': 1000, 'gamma': 0.1}
0.399 (+/-0.102) for {'kernel': 'rbf', 'C': 1000, 'gamma': 0.5}
0.452 (+/-0.052) for {'kernel': 'rbf', 'C': 1000, 'gamma': 0.01}
0.447 (+/-0.050) for {'kernel': 'rbf', 'C': 1000, 'gamma': 0.03}
0.441 (+/-0.057) for {'kernel': 'rbf', 'C': 1000, 'gamma': 0.05}
0.446 (+/-0.047) for {'kernel': 'rbf', 'C': 1000, 'gamma': 0.07}
0.453 (+/-0.060) for {'kernel': 'rbf', 'C': 1000, 'gamma': 0.001}
0.453 (+/-0.055) for {'kernel': 'rbf', 'C': 1000, 'gamma': 0.003}
0.451 (+/-0.051) for {'kernel': 'rbf', 'C': 1000, 'gamma': 0.005}
0.452 (+/-0.048) for {'kernel': 'rbf', 'C': 1000, 'gamma': 0.007}
0.459 (+/-0.050) for {'kernel': 'rbf', 'C': 1000, 'gamma': 0.0001}
0.454 (+/-0.064) for {'kernel': 'linear', 'C': 0.025}
0.456 (+/-0.066) for {'kernel': 'linear', 'C': 1}
0.455 (+/-0.065) for {'kernel': 'linear', 'C': 10}
0.454 (+/-0.065) for {'kernel': 'linear', 'C': 100}
0.453 (+/-0.067) for {'kernel': 'linear', 'C': 1000}
()
Detailed classification report:
()
The model is trained on the full development set.
The scores are computed on the full evaluation set.
()
             precision    recall  f1-score   support

          0       0.61      0.52      0.56       126
          1       0.41      0.61      0.49       146
          2       0.48      0.23      0.31       146
          3       0.41      0.48      0.44       149

avg / total       0.47      0.46      0.45       567

()
# Tuning hyper-parameters for f1
()
/usr/local/lib/python2.7/site-packages/sklearn/metrics/classification.py:1113: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)
Best parameters set found on development set:
()
{'kernel': 'rbf', 'C': 1, 'gamma': 0.001}
()
Grid scores on development set:
()
0.397 (+/-0.041) for {'kernel': 'rbf', 'C': 0.025, 'gamma': 0.1}
0.280 (+/-0.029) for {'kernel': 'rbf', 'C': 0.025, 'gamma': 0.5}
0.381 (+/-0.043) for {'kernel': 'rbf', 'C': 0.025, 'gamma': 0.01}
0.433 (+/-0.042) for {'kernel': 'rbf', 'C': 0.025, 'gamma': 0.03}
0.433 (+/-0.041) for {'kernel': 'rbf', 'C': 0.025, 'gamma': 0.05}
0.429 (+/-0.045) for {'kernel': 'rbf', 'C': 0.025, 'gamma': 0.07}
0.153 (+/-0.026) for {'kernel': 'rbf', 'C': 0.025, 'gamma': 0.001}
0.239 (+/-0.005) for {'kernel': 'rbf', 'C': 0.025, 'gamma': 0.003}
0.252 (+/-0.026) for {'kernel': 'rbf', 'C': 0.025, 'gamma': 0.005}
0.312 (+/-0.042) for {'kernel': 'rbf', 'C': 0.025, 'gamma': 0.007}
0.104 (+/-0.000) for {'kernel': 'rbf', 'C': 0.025, 'gamma': 0.0001}
0.453 (+/-0.055) for {'kernel': 'rbf', 'C': 1, 'gamma': 0.1}
0.455 (+/-0.068) for {'kernel': 'rbf', 'C': 1, 'gamma': 0.5}
0.452 (+/-0.049) for {'kernel': 'rbf', 'C': 1, 'gamma': 0.01}
0.458 (+/-0.047) for {'kernel': 'rbf', 'C': 1, 'gamma': 0.03}
0.455 (+/-0.054) for {'kernel': 'rbf', 'C': 1, 'gamma': 0.05}
0.454 (+/-0.053) for {'kernel': 'rbf', 'C': 1, 'gamma': 0.07}
0.459 (+/-0.051) for {'kernel': 'rbf', 'C': 1, 'gamma': 0.001}
0.453 (+/-0.046) for {'kernel': 'rbf', 'C': 1, 'gamma': 0.003}
0.454 (+/-0.049) for {'kernel': 'rbf', 'C': 1, 'gamma': 0.005}
0.453 (+/-0.051) for {'kernel': 'rbf', 'C': 1, 'gamma': 0.007}
0.251 (+/-0.014) for {'kernel': 'rbf', 'C': 1, 'gamma': 0.0001}
0.446 (+/-0.063) for {'kernel': 'rbf', 'C': 10, 'gamma': 0.1}
0.440 (+/-0.079) for {'kernel': 'rbf', 'C': 10, 'gamma': 0.5}
0.456 (+/-0.051) for {'kernel': 'rbf', 'C': 10, 'gamma': 0.01}
0.454 (+/-0.053) for {'kernel': 'rbf', 'C': 10, 'gamma': 0.03}
0.452 (+/-0.051) for {'kernel': 'rbf', 'C': 10, 'gamma': 0.05}
0.450 (+/-0.054) for {'kernel': 'rbf', 'C': 10, 'gamma': 0.07}
0.458 (+/-0.046) for {'kernel': 'rbf', 'C': 10, 'gamma': 0.001}
0.454 (+/-0.048) for {'kernel': 'rbf', 'C': 10, 'gamma': 0.003}
0.451 (+/-0.051) for {'kernel': 'rbf', 'C': 10, 'gamma': 0.005}
0.452 (+/-0.051) for {'kernel': 'rbf', 'C': 10, 'gamma': 0.007}
0.456 (+/-0.057) for {'kernel': 'rbf', 'C': 10, 'gamma': 0.0001}
0.450 (+/-0.043) for {'kernel': 'rbf', 'C': 100, 'gamma': 0.1}
0.408 (+/-0.092) for {'kernel': 'rbf', 'C': 100, 'gamma': 0.5}
0.454 (+/-0.048) for {'kernel': 'rbf', 'C': 100, 'gamma': 0.01}
0.454 (+/-0.047) for {'kernel': 'rbf', 'C': 100, 'gamma': 0.03}
0.448 (+/-0.047) for {'kernel': 'rbf', 'C': 100, 'gamma': 0.05}
0.445 (+/-0.064) for {'kernel': 'rbf', 'C': 100, 'gamma': 0.07}
0.450 (+/-0.037) for {'kernel': 'rbf', 'C': 100, 'gamma': 0.001}
0.454 (+/-0.057) for {'kernel': 'rbf', 'C': 100, 'gamma': 0.003}
0.455 (+/-0.051) for {'kernel': 'rbf', 'C': 100, 'gamma': 0.005}
0.456 (+/-0.052) for {'kernel': 'rbf', 'C': 100, 'gamma': 0.007}
0.452 (+/-0.061) for {'kernel': 'rbf', 'C': 100, 'gamma': 0.0001}
0.437 (+/-0.049) for {'kernel': 'rbf', 'C': 1000, 'gamma': 0.1}
0.395 (+/-0.100) for {'kernel': 'rbf', 'C': 1000, 'gamma': 0.5}
0.453 (+/-0.052) for {'kernel': 'rbf', 'C': 1000, 'gamma': 0.01}
0.448 (+/-0.049) for {'kernel': 'rbf', 'C': 1000, 'gamma': 0.03}
0.443 (+/-0.058) for {'kernel': 'rbf', 'C': 1000, 'gamma': 0.05}
0.448 (+/-0.046) for {'kernel': 'rbf', 'C': 1000, 'gamma': 0.07}
0.451 (+/-0.060) for {'kernel': 'rbf', 'C': 1000, 'gamma': 0.001}
0.454 (+/-0.056) for {'kernel': 'rbf', 'C': 1000, 'gamma': 0.003}
0.452 (+/-0.052) for {'kernel': 'rbf', 'C': 1000, 'gamma': 0.005}
0.453 (+/-0.049) for {'kernel': 'rbf', 'C': 1000, 'gamma': 0.007}
0.457 (+/-0.049) for {'kernel': 'rbf', 'C': 1000, 'gamma': 0.0001}
0.452 (+/-0.060) for {'kernel': 'linear', 'C': 0.025}
0.454 (+/-0.062) for {'kernel': 'linear', 'C': 1}
0.453 (+/-0.061) for {'kernel': 'linear', 'C': 10}
0.453 (+/-0.061) for {'kernel': 'linear', 'C': 100}
0.451 (+/-0.063) for {'kernel': 'linear', 'C': 1000}
()
Detailed classification report:
()
The model is trained on the full development set.
The scores are computed on the full evaluation set.
()
             precision    recall  f1-score   support

          0       0.57      0.56      0.56       126
          1       0.42      0.55      0.47       146
          2       0.46      0.32      0.38       146
          3       0.42      0.43      0.43       149

avg / total       0.46      0.46      0.46       567

()
➜  cmup git:(master) ✗ 
