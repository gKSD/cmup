➜  cmup git:(master) ✗ ./cmup.py trainEmotionClassifier --load
[CONFIG] reading config: './conf/cmup_conf.cfg'
[CLASSIFIER] loading train data
[LOADER] label: Angry_all
[LOADER] n features: 160
[LOADER] m samples: 507
[LOADER] label: Happy_all
[LOADER] n features: 160
[LOADER] m samples: 584
[LOADER] label: Relax_all
[LOADER] n features: 160
[LOADER] m samples: 584
[LOADER] label: Sad_all
[LOADER] n features: 160
[LOADER] m samples: 597
[LOADER] making feature matrix and feature labels vector
[LOADER] feature matrix shape: 2272 x 160
[LOADER] feature labels shape: 2272
[CLASSIFIER] train data shape
(2272, 160)
[CLASSIFIER] loading test data
[LOADER] label: Angry_all
[LOADER] n features: 160
[LOADER] m samples: 126
[LOADER] label: Happy_all
[LOADER] n features: 160
[LOADER] m samples: 146
[LOADER] label: Relax_all
[LOADER] n features: 160
[LOADER] m samples: 146
[LOADER] label: Sad_all
[LOADER] n features: 160
[LOADER] m samples: 149
[LOADER] making feature matrix and feature labels vector
[LOADER] feature matrix shape: 567 x 160
[LOADER] feature labels shape: 567
[CLASSIFIER] test data shape
(567, 160)
[LOADER] perform feature standartization (using sklearn.preprocessing.StandardScaler class)
[[  78.3966217    36.00850677    0.97978675 ...,    2.79889989    0.          101.        ]
 [ 120.2877655    11.91337967    0.96864593 ...,    3.96510816    0.           89.        ]
 [ 104.6809082    18.70925331    0.93943554 ...,    4.26911879    0.          126.        ]
 ..., 
 [  23.54691124   11.5001936     0.87458295 ...,    4.20241404    0.           98.        ]
 [  48.5058136    21.2920208     0.96427035 ...,    2.93672514    0.          157.        ]
 [ 110.03178406   15.44882202    0.63834012 ...,    3.39472437    0.          144.        ]]
[[-0.08256581  2.14064642  0.56590879 ..., -0.70583633  0.         -0.88586098]
 [ 0.66968071 -0.80162041  0.49203325 ...,  0.42101711  0.         -1.36385563]
 [ 0.38942567  0.02822678  0.29833724 ...,  0.7147686   0.          0.10996121]
 ..., 
 [-1.06751155 -0.85207475 -0.1317046  ...,  0.65031487  0.         -1.00535964]
 [-0.61932022  0.34360968  0.46301851 ..., -0.57266213  0.          1.34478073]
 [ 0.48551228 -0.36990595 -1.6982464  ..., -0.13011852  0.          0.82695319]]
[LOADER] perform feature standartization (using sklearn.preprocessing.StandardScaler class)
[[  44.66075897   11.60788822    0.97211963 ...,    5.06467581    0.          157.        ]
 [  30.50131989   12.40735054    0.68405718 ...,    2.08291769    0.           99.        ]
 [   6.62777519    8.3523035     0.67839611 ...,    2.05117822    0.           99.        ]
 ..., 
 [  59.63494492   11.99690628    0.94270056 ...,    4.19834995    0.          161.        ]
 [  17.15740776   14.6537571     0.96147597 ...,    1.36824691    0.          129.        ]
 [  68.04239655   18.75434494    0.97158396 ...,    2.94542265    0.          129.        ]]
[[-0.68836653 -0.83892411  0.51506758 ...,  1.48347878  0.          1.34478073]
 [-0.94263003 -0.74130141 -1.39509355 ..., -1.39765701  0.         -0.96552676]
 [-1.3713314  -1.23646503 -1.43263248 ..., -1.42832541  0.         -0.96552676]
 ..., 
 [-0.41947248 -0.79142094  0.31998782 ...,  0.64638793  0.          1.50411228]
 [-1.18224897 -0.46699169  0.44448876 ..., -2.08821053  0.          0.22945987]
 [-0.26849842  0.03373294  0.51151554 ..., -0.56425812  0.          0.22945987]]
[LOADER] perform PCA feature selction [n_components: 2]
[LABEL] PCA Explained Variance: [ 0.12303868  0.08240316]
[[ -5.78645634  -1.81071004]
 [  0.34433293   2.52821233]
 [ -2.54943935   5.38100214]
 [ 12.66623476   0.77826386]
 [  2.72758096  -3.20389513]]
[LOADER] perform PCA feature selction [n_components: 2]
[LABEL] PCA Explained Variance: [ 0.12303868  0.08240316]
[[  4.44901300e+00  -1.26445293e-03]
 [  3.65271530e+00  -6.73040249e-01]
 [  1.29298152e+01  -5.95780427e+00]
 [  5.36757088e+00   5.50385980e+00]
 [ -1.23197750e+00  -6.48411748e+00]]
[LOADER] perform label encoding
['Re' 'Re' 'Re' ..., 'Sa' 'Sa' 'Sa']
[2 2 2 ..., 3 3 3]
[LOADER] perform label encoding
['Re' 'Re' 'Re' 'Re' 'Re' 'Re' 'Re' 'Re' 'Re' 'Re' 'Re' 'Re' 'Re' 'Re' 'Re'
 'Re' 'Re' 'Re' 'Re' 'Re' 'Re' 'Re' 'Re' 'Re' 'Re' 'Re' 'Re' 'Re' 'Re' 'Re'
 'Re' 'Re' 'Re' 'Re' 'Re' 'Re' 'Re' 'Re' 'Re' 'Re' 'Re' 'Re' 'Re' 'Re' 'Re'
 'Re' 'Re' 'Re' 'Re' 'Re' 'Re' 'Re' 'Re' 'Re' 'Re' 'Re' 'Re' 'Re' 'Re' 'Re'
 'Re' 'Re' 'Re' 'Re' 'Re' 'Re' 'Re' 'Re' 'Re' 'Re' 'Re' 'Re' 'Re' 'Re' 'Re'
 'Re' 'Re' 'Re' 'Re' 'Re' 'Re' 'Re' 'Re' 'Re' 'Re' 'Re' 'Re' 'Re' 'Re' 'Re'
 'Re' 'Re' 'Re' 'Re' 'Re' 'Re' 'Re' 'Re' 'Re' 'Re' 'Re' 'Re' 'Re' 'Re' 'Re'
 'Re' 'Re' 'Re' 'Re' 'Re' 'Re' 'Re' 'Re' 'Re' 'Re' 'Re' 'Re' 'Re' 'Re' 'Re'
 'Re' 'Re' 'Re' 'Re' 'Re' 'Re' 'Re' 'Re' 'Re' 'Re' 'Re' 'Re' 'Re' 'Re' 'Re'
 'Re' 'Re' 'Re' 'Re' 'Re' 'Re' 'Re' 'Re' 'Re' 'Re' 'Re' 'Ha' 'Ha' 'Ha' 'Ha'
 'Ha' 'Ha' 'Ha' 'Ha' 'Ha' 'Ha' 'Ha' 'Ha' 'Ha' 'Ha' 'Ha' 'Ha' 'Ha' 'Ha' 'Ha'
 'Ha' 'Ha' 'Ha' 'Ha' 'Ha' 'Ha' 'Ha' 'Ha' 'Ha' 'Ha' 'Ha' 'Ha' 'Ha' 'Ha' 'Ha'
 'Ha' 'Ha' 'Ha' 'Ha' 'Ha' 'Ha' 'Ha' 'Ha' 'Ha' 'Ha' 'Ha' 'Ha' 'Ha' 'Ha' 'Ha'
 'Ha' 'Ha' 'Ha' 'Ha' 'Ha' 'Ha' 'Ha' 'Ha' 'Ha' 'Ha' 'Ha' 'Ha' 'Ha' 'Ha' 'Ha'
 'Ha' 'Ha' 'Ha' 'Ha' 'Ha' 'Ha' 'Ha' 'Ha' 'Ha' 'Ha' 'Ha' 'Ha' 'Ha' 'Ha' 'Ha'
 'Ha' 'Ha' 'Ha' 'Ha' 'Ha' 'Ha' 'Ha' 'Ha' 'Ha' 'Ha' 'Ha' 'Ha' 'Ha' 'Ha' 'Ha'
 'Ha' 'Ha' 'Ha' 'Ha' 'Ha' 'Ha' 'Ha' 'Ha' 'Ha' 'Ha' 'Ha' 'Ha' 'Ha' 'Ha' 'Ha'
 'Ha' 'Ha' 'Ha' 'Ha' 'Ha' 'Ha' 'Ha' 'Ha' 'Ha' 'Ha' 'Ha' 'Ha' 'Ha' 'Ha' 'Ha'
 'Ha' 'Ha' 'Ha' 'Ha' 'Ha' 'Ha' 'Ha' 'Ha' 'Ha' 'Ha' 'Ha' 'Ha' 'Ha' 'Ha' 'Ha'
 'Ha' 'Ha' 'Ha' 'Ha' 'Ha' 'Ha' 'Ha' 'An' 'An' 'An' 'An' 'An' 'An' 'An' 'An'
 'An' 'An' 'An' 'An' 'An' 'An' 'An' 'An' 'An' 'An' 'An' 'An' 'An' 'An' 'An'
 'An' 'An' 'An' 'An' 'An' 'An' 'An' 'An' 'An' 'An' 'An' 'An' 'An' 'An' 'An'
 'An' 'An' 'An' 'An' 'An' 'An' 'An' 'An' 'An' 'An' 'An' 'An' 'An' 'An' 'An'
 'An' 'An' 'An' 'An' 'An' 'An' 'An' 'An' 'An' 'An' 'An' 'An' 'An' 'An' 'An'
 'An' 'An' 'An' 'An' 'An' 'An' 'An' 'An' 'An' 'An' 'An' 'An' 'An' 'An' 'An'
 'An' 'An' 'An' 'An' 'An' 'An' 'An' 'An' 'An' 'An' 'An' 'An' 'An' 'An' 'An'
 'An' 'An' 'An' 'An' 'An' 'An' 'An' 'An' 'An' 'An' 'An' 'An' 'An' 'An' 'An'
 'An' 'An' 'An' 'An' 'An' 'An' 'An' 'An' 'An' 'An' 'An' 'An' 'An' 'Sa' 'Sa'
 'Sa' 'Sa' 'Sa' 'Sa' 'Sa' 'Sa' 'Sa' 'Sa' 'Sa' 'Sa' 'Sa' 'Sa' 'Sa' 'Sa' 'Sa'
 'Sa' 'Sa' 'Sa' 'Sa' 'Sa' 'Sa' 'Sa' 'Sa' 'Sa' 'Sa' 'Sa' 'Sa' 'Sa' 'Sa' 'Sa'
 'Sa' 'Sa' 'Sa' 'Sa' 'Sa' 'Sa' 'Sa' 'Sa' 'Sa' 'Sa' 'Sa' 'Sa' 'Sa' 'Sa' 'Sa'
 'Sa' 'Sa' 'Sa' 'Sa' 'Sa' 'Sa' 'Sa' 'Sa' 'Sa' 'Sa' 'Sa' 'Sa' 'Sa' 'Sa' 'Sa'
 'Sa' 'Sa' 'Sa' 'Sa' 'Sa' 'Sa' 'Sa' 'Sa' 'Sa' 'Sa' 'Sa' 'Sa' 'Sa' 'Sa' 'Sa'
 'Sa' 'Sa' 'Sa' 'Sa' 'Sa' 'Sa' 'Sa' 'Sa' 'Sa' 'Sa' 'Sa' 'Sa' 'Sa' 'Sa' 'Sa'
 'Sa' 'Sa' 'Sa' 'Sa' 'Sa' 'Sa' 'Sa' 'Sa' 'Sa' 'Sa' 'Sa' 'Sa' 'Sa' 'Sa' 'Sa'
 'Sa' 'Sa' 'Sa' 'Sa' 'Sa' 'Sa' 'Sa' 'Sa' 'Sa' 'Sa' 'Sa' 'Sa' 'Sa' 'Sa' 'Sa'
 'Sa' 'Sa' 'Sa' 'Sa' 'Sa' 'Sa' 'Sa' 'Sa' 'Sa' 'Sa' 'Sa' 'Sa' 'Sa' 'Sa' 'Sa'
 'Sa' 'Sa' 'Sa' 'Sa' 'Sa' 'Sa' 'Sa' 'Sa' 'Sa' 'Sa' 'Sa' 'Sa']
[2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2
 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2
 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2
 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1 1
 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3
 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3
 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3
 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3
 3 3 3 3 3 3 3 3 3 3 3 3]
# Tuning hyper-parameters for precision
()
/usr/local/lib/python2.7/site-packages/sklearn/metrics/classification.py:1113: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)
Best parameters set found on development set:
()
{'kernel': 'rbf', 'C': 1000, 'gamma': 0.005}
()
Grid scores on development set:
()
0.376 (+/-0.099) for {'kernel': 'rbf', 'C': 0.025, 'gamma': 0.01}
0.370 (+/-0.105) for {'kernel': 'rbf', 'C': 0.025, 'gamma': 0.05}
0.153 (+/-0.031) for {'kernel': 'rbf', 'C': 0.025, 'gamma': 0.001}
0.317 (+/-0.221) for {'kernel': 'rbf', 'C': 0.025, 'gamma': 0.005}
0.066 (+/-0.000) for {'kernel': 'rbf', 'C': 0.025, 'gamma': 0.0001}
0.479 (+/-0.059) for {'kernel': 'rbf', 'C': 1, 'gamma': 0.01}
0.482 (+/-0.061) for {'kernel': 'rbf', 'C': 1, 'gamma': 0.05}
0.458 (+/-0.071) for {'kernel': 'rbf', 'C': 1, 'gamma': 0.001}
0.471 (+/-0.067) for {'kernel': 'rbf', 'C': 1, 'gamma': 0.005}
0.370 (+/-0.183) for {'kernel': 'rbf', 'C': 1, 'gamma': 0.0001}
0.482 (+/-0.049) for {'kernel': 'rbf', 'C': 10, 'gamma': 0.01}
0.476 (+/-0.062) for {'kernel': 'rbf', 'C': 10, 'gamma': 0.05}
0.471 (+/-0.063) for {'kernel': 'rbf', 'C': 10, 'gamma': 0.001}
0.482 (+/-0.060) for {'kernel': 'rbf', 'C': 10, 'gamma': 0.005}
0.449 (+/-0.072) for {'kernel': 'rbf', 'C': 10, 'gamma': 0.0001}
0.486 (+/-0.065) for {'kernel': 'rbf', 'C': 100, 'gamma': 0.01}
0.484 (+/-0.078) for {'kernel': 'rbf', 'C': 100, 'gamma': 0.05}
0.468 (+/-0.061) for {'kernel': 'rbf', 'C': 100, 'gamma': 0.001}
0.487 (+/-0.056) for {'kernel': 'rbf', 'C': 100, 'gamma': 0.005}
0.454 (+/-0.070) for {'kernel': 'rbf', 'C': 100, 'gamma': 0.0001}
0.484 (+/-0.065) for {'kernel': 'rbf', 'C': 1000, 'gamma': 0.01}
0.473 (+/-0.074) for {'kernel': 'rbf', 'C': 1000, 'gamma': 0.05}
0.481 (+/-0.058) for {'kernel': 'rbf', 'C': 1000, 'gamma': 0.001}
0.489 (+/-0.049) for {'kernel': 'rbf', 'C': 1000, 'gamma': 0.005}
0.470 (+/-0.066) for {'kernel': 'rbf', 'C': 1000, 'gamma': 0.0001}
0.447 (+/-0.066) for {'kernel': 'linear', 'C': 0.025}
0.447 (+/-0.066) for {'kernel': 'linear', 'C': 1}
0.447 (+/-0.066) for {'kernel': 'linear', 'C': 10}
0.447 (+/-0.066) for {'kernel': 'linear', 'C': 100}
0.446 (+/-0.065) for {'kernel': 'linear', 'C': 1000}
()
Detailed classification report:
()
The model is trained on the full development set.
The scores are computed on the full evaluation set.
()
             precision    recall  f1-score   support

          0       0.68      0.50      0.58       126
          1       0.45      0.67      0.54       146
          2       0.47      0.44      0.45       146
          3       0.50      0.40      0.44       149

avg / total       0.52      0.50      0.50       567

()
# Tuning hyper-parameters for recall
()
Best parameters set found on development set:
()
{'kernel': 'rbf', 'C': 100, 'gamma': 0.05}
()
Grid scores on development set:
()
0.418 (+/-0.061) for {'kernel': 'rbf', 'C': 0.025, 'gamma': 0.01}
0.421 (+/-0.059) for {'kernel': 'rbf', 'C': 0.025, 'gamma': 0.05}
0.296 (+/-0.028) for {'kernel': 'rbf', 'C': 0.025, 'gamma': 0.001}
0.370 (+/-0.037) for {'kernel': 'rbf', 'C': 0.025, 'gamma': 0.005}
0.250 (+/-0.000) for {'kernel': 'rbf', 'C': 0.025, 'gamma': 0.0001}
0.452 (+/-0.057) for {'kernel': 'rbf', 'C': 1, 'gamma': 0.01}
0.456 (+/-0.056) for {'kernel': 'rbf', 'C': 1, 'gamma': 0.05}
0.455 (+/-0.074) for {'kernel': 'rbf', 'C': 1, 'gamma': 0.001}
0.448 (+/-0.063) for {'kernel': 'rbf', 'C': 1, 'gamma': 0.005}
0.365 (+/-0.031) for {'kernel': 'rbf', 'C': 1, 'gamma': 0.0001}
0.450 (+/-0.049) for {'kernel': 'rbf', 'C': 10, 'gamma': 0.01}
0.452 (+/-0.059) for {'kernel': 'rbf', 'C': 10, 'gamma': 0.05}
0.451 (+/-0.065) for {'kernel': 'rbf', 'C': 10, 'gamma': 0.001}
0.452 (+/-0.057) for {'kernel': 'rbf', 'C': 10, 'gamma': 0.005}
0.450 (+/-0.078) for {'kernel': 'rbf', 'C': 10, 'gamma': 0.0001}
0.455 (+/-0.056) for {'kernel': 'rbf', 'C': 100, 'gamma': 0.01}
0.457 (+/-0.070) for {'kernel': 'rbf', 'C': 100, 'gamma': 0.05}
0.442 (+/-0.059) for {'kernel': 'rbf', 'C': 100, 'gamma': 0.001}
0.451 (+/-0.056) for {'kernel': 'rbf', 'C': 100, 'gamma': 0.005}
0.452 (+/-0.074) for {'kernel': 'rbf', 'C': 100, 'gamma': 0.0001}
0.451 (+/-0.060) for {'kernel': 'rbf', 'C': 1000, 'gamma': 0.01}
0.450 (+/-0.069) for {'kernel': 'rbf', 'C': 1000, 'gamma': 0.05}
0.447 (+/-0.055) for {'kernel': 'rbf', 'C': 1000, 'gamma': 0.001}
0.454 (+/-0.041) for {'kernel': 'rbf', 'C': 1000, 'gamma': 0.005}
0.449 (+/-0.065) for {'kernel': 'rbf', 'C': 1000, 'gamma': 0.0001}
0.449 (+/-0.072) for {'kernel': 'linear', 'C': 0.025}
0.449 (+/-0.072) for {'kernel': 'linear', 'C': 1}
0.449 (+/-0.072) for {'kernel': 'linear', 'C': 10}
0.448 (+/-0.071) for {'kernel': 'linear', 'C': 100}
0.448 (+/-0.071) for {'kernel': 'linear', 'C': 1000}
()
Detailed classification report:
()
The model is trained on the full development set.
The scores are computed on the full evaluation set.
()
             precision    recall  f1-score   support

          0       0.66      0.52      0.58       126
          1       0.45      0.64      0.53       146
          2       0.47      0.42      0.45       146
          3       0.47      0.40      0.43       149

avg / total       0.51      0.50      0.49       567

()
# Tuning hyper-parameters for f1
()
/usr/local/lib/python2.7/site-packages/sklearn/metrics/classification.py:1113: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)
Best parameters set found on development set:
()
{'kernel': 'rbf', 'C': 100, 'gamma': 0.05}
()
Grid scores on development set:
()
0.362 (+/-0.059) for {'kernel': 'rbf', 'C': 0.025, 'gamma': 0.01}
0.372 (+/-0.061) for {'kernel': 'rbf', 'C': 0.025, 'gamma': 0.05}
0.189 (+/-0.027) for {'kernel': 'rbf', 'C': 0.025, 'gamma': 0.001}
0.273 (+/-0.064) for {'kernel': 'rbf', 'C': 0.025, 'gamma': 0.005}
0.104 (+/-0.000) for {'kernel': 'rbf', 'C': 0.025, 'gamma': 0.0001}
0.454 (+/-0.058) for {'kernel': 'rbf', 'C': 1, 'gamma': 0.01}
0.458 (+/-0.056) for {'kernel': 'rbf', 'C': 1, 'gamma': 0.05}
0.453 (+/-0.071) for {'kernel': 'rbf', 'C': 1, 'gamma': 0.001}
0.448 (+/-0.065) for {'kernel': 'rbf', 'C': 1, 'gamma': 0.005}
0.265 (+/-0.050) for {'kernel': 'rbf', 'C': 1, 'gamma': 0.0001}
0.451 (+/-0.050) for {'kernel': 'rbf', 'C': 10, 'gamma': 0.01}
0.454 (+/-0.060) for {'kernel': 'rbf', 'C': 10, 'gamma': 0.05}
0.452 (+/-0.066) for {'kernel': 'rbf', 'C': 10, 'gamma': 0.001}
0.454 (+/-0.058) for {'kernel': 'rbf', 'C': 10, 'gamma': 0.005}
0.446 (+/-0.073) for {'kernel': 'rbf', 'C': 10, 'gamma': 0.0001}
0.457 (+/-0.057) for {'kernel': 'rbf', 'C': 100, 'gamma': 0.01}
0.460 (+/-0.072) for {'kernel': 'rbf', 'C': 100, 'gamma': 0.05}
0.441 (+/-0.060) for {'kernel': 'rbf', 'C': 100, 'gamma': 0.001}
0.453 (+/-0.058) for {'kernel': 'rbf', 'C': 100, 'gamma': 0.005}
0.450 (+/-0.072) for {'kernel': 'rbf', 'C': 100, 'gamma': 0.0001}
0.453 (+/-0.062) for {'kernel': 'rbf', 'C': 1000, 'gamma': 0.01}
0.452 (+/-0.070) for {'kernel': 'rbf', 'C': 1000, 'gamma': 0.05}
0.447 (+/-0.057) for {'kernel': 'rbf', 'C': 1000, 'gamma': 0.001}
0.456 (+/-0.042) for {'kernel': 'rbf', 'C': 1000, 'gamma': 0.005}
0.449 (+/-0.065) for {'kernel': 'rbf', 'C': 1000, 'gamma': 0.0001}
0.445 (+/-0.068) for {'kernel': 'linear', 'C': 0.025}
0.445 (+/-0.067) for {'kernel': 'linear', 'C': 1}
0.445 (+/-0.067) for {'kernel': 'linear', 'C': 10}
0.444 (+/-0.067) for {'kernel': 'linear', 'C': 100}
0.444 (+/-0.066) for {'kernel': 'linear', 'C': 1000}
()
Detailed classification report:
()
The model is trained on the full development set.
The scores are computed on the full evaluation set.
()
             precision    recall  f1-score   support

          0       0.66      0.52      0.58       126
          1       0.45      0.64      0.53       146
          2       0.47      0.42      0.45       146
          3       0.47      0.40      0.43       149

avg / total       0.51      0.50      0.49       567

()
